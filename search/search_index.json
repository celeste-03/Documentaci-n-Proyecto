{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MODELO SENCILLO DE UN CAPACITOR Y SU SOLUCI\u00d3N NUM\u00c9RICA (ELECTROST\u00c1TICA)","text":""},{"location":"#coordinadores-marlon-brenes-y-federico-munoz","title":"Coordinadores: Marlon Brenes y Federico Mu\u00f1oz","text":""},{"location":"#estudiantes","title":"Estudiantes","text":"<ul> <li>Carlos Echandi Jara (B92687)</li> <li>Natalia Iannarella Arguedas (A83166)</li> <li>\u00c1ngel Fabricio Aguirre Berm\u00fadez (C10152)</li> <li>Mar\u00eda Celeste Ure\u00f1a Sand\u00ed (B98009)</li> </ul> <p>Bienvenido, este proyecto pretende resolver la ecuacion de Laplace aplicada a un capacitor electr\u00f3nico ideal.</p> <p>Esta p\u00e1gina tiene como objetivo documentar el c\u00f3digo presente en el repositorio Proyecto_Electromagnetismo.</p> <p>Se resuelve la ecuaci\u00f3n de Laplace en dos dimensiones para el potencial electrost\u00e1tico</p> <p>\u03c6 = \u03c6(x, y)</p> <p></p> <p>En una placa cuadrada de 10 cm \u00d7 10 cm . El problema modela de forma ideal el capacitor electronico que se presenta en la siguiente figura:</p> <p></p>"},{"location":"#descripcion","title":"Descripci\u00f3n","text":"<p>Este proyecto incluye:</p> <ul> <li>M\u00e9todos num\u00e9ricos para resolver ecuaciones diferenciales parciales.</li> <li>Implementaciones en Python y C++.</li> <li>Paralelizaci\u00f3n con OpenMP y MPI.</li> <li>Visualizaci\u00f3n de resultados y an\u00e1lisis num\u00e9rico.</li> </ul>"},{"location":"codigo_cpp/Gauss_Seidel/","title":"Soluci\u00f3n de la Ecuaci\u00f3n de Laplace en C++","text":"<p>Aqui se presenta una traducci\u00f3n del c\u00f3digo implementado en python. Esto con el objetivo de mejorar su velocidad, y posteriormente paralelizar.</p>"},{"location":"codigo_cpp/Gauss_Seidel/#codigo-fuente","title":"C\u00f3digo Fuente","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;cmath&gt; // Para std::abs\n#include &lt;algorithm&gt; // Para std::max\n\ndouble calcular_delta(const std::vector&lt;double&gt; &amp;phi, const std::vector&lt;double&gt; &amp;phi_copy) {\n    double delta = 0.0;\n    for (size_t i = 0; i &lt; phi.size(); ++i) {\n        delta = std::max(delta, std::abs(phi[i] - phi_copy[i]));\n    }\n    return delta;\n}\n\nint main(int argc, char* argv[]) {\n    if (argc != 7) {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" --N [Tama\u00f1o lineal de la grilla cuadrada (NxN)] --t [Tolerancia] --L [Tama\u00f1o lineal del capacitor cuadrado (LxL)]\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    std::cout &lt;&lt; \"Bienvenido.\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Se esta calculando la solucion de la ecuacion de Laplace bajo los parametros definidos...\" &lt;&lt; std::endl;\n\n    int N = atoi(argv[2]);  // Tama\u00f1o de la grilla (NxN)\n    double tolerance = atof(argv[4]);  // Tolerancia\n    int L = atoi(argv[6]);  // Tama\u00f1o lineal del capacitor fisico\n    int grilla_size = N + 1;  // Tama\u00f1o total de la grilla (incluye frontera)\n\n    std::vector&lt;double&gt; phi(grilla_size * grilla_size, 0.0);\n\n    int start_y = (N/L) * 2.0;  // Coordenada inicial (y = 2cm)\n    int end_y = (N/L) * 8.0;    // Coordenada final (y= 8cm)\n    int plate_x1 = (N / L) * 2.0; // L\u00ednea vertical izquierda (x = 2cm)\n    int plate_x2 = (N / L) * 8.0; // L\u00ednea vertical derecha (x = 8cm)\n\n    for (int i = start_y; i &lt; end_y; ++i) {\n        phi[i * grilla_size + plate_x1] = 1.0;  // +1V en la placa izquierda\n        phi[i * grilla_size + plate_x2] = -1.0; // -1V en la placa derecha\n    }\n\n    std::vector&lt;double&gt; phi_copy = phi;\n\n    double delta = 1.0;\n    int its = 0;\n\n    while (delta &gt; tolerance) {\n        its++;\n\n        for (int i = 1; i &lt; N; ++i) {\n            for (int j = 1; j &lt; N; ++j) {\n                phi[i * grilla_size + j] = (1.0 / 4.0) *\n                    (phi[(i + 1) * grilla_size + j] +\n                     phi[(i - 1) * grilla_size + j] +\n                     phi[i * grilla_size + (j + 1)] +\n                     phi[i * grilla_size + (j - 1)]);\n            }\n        }\n\n        for (int i = start_y; i &lt; end_y; ++i) {\n            phi[i * grilla_size + plate_x1] = 1.0;\n            phi[i * grilla_size + plate_x2] = -1.0;\n        }\n\n        delta = calcular_delta(phi, phi_copy);\n\n        phi_copy = phi;\n    }\n\n    std::cout &lt;&lt; \"Se llego a la tolerancia tras \" &lt;&lt; its &lt;&lt; \" iteraciones.\" &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre> <p>Se logra la convergencia tras 1196 iteraciones (las mismas que presenta su contraparte en python).</p>"},{"location":"codigo_cpp/Gauss_Seidel_Dist/","title":"Soluci\u00f3n de la Ecuaci\u00f3n de Laplace en C++ en memoria compartida","text":"<p>Este c\u00f3digo pretende utilizar MPI para lograr una paralelizaci\u00f3n en memoria compartida. En resumen cada proceso se queda con una banda horizontal de la placa, esta banda esta compuesta por una regi\u00f3n sobre la que itera, y los valores colindantes. Luego, los procesos con <code>rank</code> par se comunican los de <code>rank</code> impar para actualizar dichos valores colindantes e iterar de manera independiente.</p>"},{"location":"codigo_cpp/Gauss_Seidel_Dist/#codigo-fuente","title":"C\u00f3digo Fuente","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;cmath&gt;\n#include &lt;algorithm&gt;\n#include &lt;mpi.h&gt;\n\ndouble calcular_delta(const std::vector&lt;double&gt; &amp;phi, const std::vector&lt;double&gt; &amp;phi_copy) {\n    double delta = 0.0;\n    for (size_t i = 0; i &lt; phi.size(); ++i) {\n        delta = std::max(delta, std::abs(phi[i] - phi_copy[i]));\n    }\n    return delta;\n}\n\n\nvoid iteracion_local(std::vector&lt;double&gt; &amp;phi, int local_rows, int grilla_size) {\n    for (int i = 1; i &lt;= local_rows; ++i) {\n        for (int j = 1; j &lt; grilla_size - 1; ++j) {\n            phi[i * grilla_size + j] = 0.25 * (\n                phi[(i + 1) * grilla_size + j] +  // Celda inferior (ya actualizada en esta iteraci\u00f3n)\n                phi[(i - 1) * grilla_size + j] +  // Celda superior (ya actualizada en esta iteraci\u00f3n)\n                phi[i * grilla_size + (j + 1)] +  \n                phi[i * grilla_size + (j - 1)] );   \n        }\n    }\n}\n\n\nvoid establecer_placas(std::vector&lt;double&gt; &amp;local_phi, int start_row, int end_row, int N, int L, int grilla_size) {\n    int start_y = (N / L) * 2;\n    int end_y = (N / L) * 8;\n    int plate_x1 = (N / L) * 2;\n    int plate_x2 = (N / L) * 8;\n    // Verificamos si la regi\u00f3n de filas que maneja este proceso intersecta con las coordenadas de las placas\n    if (start_row &lt;= end_y &amp;&amp; end_row &gt;= start_y) {\n        // Actualizamos las filas correspondientes en la regi\u00f3n de las placas\n        for (int i = std::max(start_row, start_y); i &lt; std::min(end_row, end_y); ++i) {\n            local_phi[(i - start_row + 1) * grilla_size + plate_x1] = 1.0;  // +1V en la placa izquierda\n            local_phi[(i - start_row + 1) * grilla_size + plate_x2] = -1.0; // -1V en la placa derecha\n        }\n    }\n}\n\n\nint main(int argc, char* argv[]) {\n\n    MPI_Init(&amp;argc, &amp;argv);\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n    if (argc != 7) {\n        if (rank == 0) {\n            std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" --N [Tama\u00f1o lineal de la grilla cuadrada (NxN)] --t [Tolerancia] --L [Tama\u00f1o lineal del capacitor cuadrado (LxL)]\" &lt;&lt; std::endl;\n        }\n        MPI_Finalize();\n        return 1;\n    }\n\n    if (rank == 0) {\n        std::cout &lt;&lt; \"Bienvenido.\" &lt;&lt; std::endl;\n        std::cout &lt;&lt; \"Se esta calculando la solucion de la ecuacion de Laplace bajo los parametros definidos...\" &lt;&lt; std::endl;\n    }\n\n    int N = atoi(argv[2]);\n    double tolerance = atof(argv[4]);\n    int L = atoi(argv[6]);\n    int grilla_size = N + 1;\n\n    double time_1 = MPI_Wtime();\n\n    // Dividir la grilla en bandas horizontales\n    int nlocal = N / size; // Filas por proceso //rows_per_proc \n    int rest = N % size; // Lo que sobra si el tama\u00f1o no es divisible entre los procesos disponibles //extra_rows \n\n    int local_rows = nlocal;  // Inicializamos con el valor b\u00e1sico\n    if (rank &lt; rest) {\n        local_rows += 1;  // Si hay resto, los primeros procesos tiene una fila adicional\n    }\n    int start_row = rank * nlocal + std::min(rank, rest); // A los \u00faltimos procesos se les suma el resto, los primeros se van acomodando con el mismmo espaciado entre ellos\n    int end_row = start_row + local_rows;\n    int local_rows_con_vecinos = local_rows + 2; // A\u00f1adir las filas solapadas //paded_rows\n\n\n    std::vector&lt;double&gt; local_phi(local_rows_con_vecinos * grilla_size, 0.0);\n    establecer_placas(local_phi, start_row, end_row, N, L, grilla_size);\n\n    std::vector&lt;double&gt; local_phi_copy = local_phi;\n\n    double global_delta = 1.0;\n    double local_delta = 1.0;\n    int its = 0;\n\n    while (global_delta &gt; tolerance) {\n        its++;\n\n        // Fase 1: Procesos pares env\u00edan, procesos impares reciben\n        if (rank % 2 == 0) { // Procesos pares env\u00edan\n            if (rank &lt; size - 1) { // Si hay un vecino abajo\n                MPI_Send(&amp;local_phi[local_rows * grilla_size], grilla_size, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD);\n            }\n            if (rank &gt; 0) { // Si hay un vecino arriba\n                MPI_Send(&amp;local_phi[grilla_size], grilla_size, MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD);\n            }\n        } else { // Procesos impares reciben\n            if (rank &gt; 0) { // Si hay un vecino arriba\n                MPI_Recv(&amp;local_phi[0], grilla_size, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n            if (rank &lt; size - 1) { // Si hay un vecino abajo\n                MPI_Recv(&amp;local_phi[(local_rows + 1) * grilla_size], grilla_size, MPI_DOUBLE, rank + 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD); // Sincronizaci\u00f3n entre env\u00edos y recepciones\n\n        // Fase 2: Procesos impares iteran localmente\n        if (rank % 2 == 1) {\n            iteracion_local(local_phi, local_rows, grilla_size);\n            establecer_placas(local_phi, start_row, end_row, N, L, grilla_size);\n        }\n\n        // Fase 3: Procesos impares env\u00edan, procesos pares reciben\n        if (rank % 2 == 1) { // Procesos impares env\u00edan\n            if (rank &lt; size - 1) { // Si hay un vecino abajo\n                MPI_Send(&amp;local_phi[local_rows * grilla_size], grilla_size, MPI_DOUBLE, rank + 1, 2, MPI_COMM_WORLD);\n            }\n            if (rank &gt; 0) { // Si hay un vecino arriba\n                MPI_Send(&amp;local_phi[grilla_size], grilla_size, MPI_DOUBLE, rank - 1, 3, MPI_COMM_WORLD);\n            }\n        } else { // Procesos pares reciben\n            if (rank &gt; 0) { // Si hay un vecino arriba\n                MPI_Recv(&amp;local_phi[0], grilla_size, MPI_DOUBLE, rank - 1, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n            if (rank &lt; size - 1) { // Si hay un vecino abajo\n                MPI_Recv(&amp;local_phi[(local_rows + 1) * grilla_size], grilla_size, MPI_DOUBLE, rank + 1, 3, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD); // Sincronizaci\u00f3n entre env\u00edos y recepciones\n\n        // Fase 4: Procesos pares actualizan e iteran\n        if (rank % 2 == 0) {\n            iteracion_local(local_phi, local_rows, grilla_size);\n            establecer_placas(local_phi, start_row, end_row, N, L, grilla_size);\n        }\n\n        // Calcular delta local\n        local_delta = calcular_delta(local_phi, local_phi_copy);\n\n        // Reducir el delta m\u00e1ximo global\n        MPI_Allreduce(&amp;local_delta, &amp;global_delta, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\n        local_phi_copy = local_phi;\n    }\n\n    double time_2 = MPI_Wtime();\n\n    if (rank == 0) {\n        std::cout &lt;&lt; \"Se lleg\u00f3 a la tolerancia tras \" &lt;&lt; its &lt;&lt; \" iteraciones.\" &lt;&lt; std::endl;\n        std::cout &lt;&lt; \"Tiempo utilizado: \" &lt;&lt; time_2 - time_1 &lt;&lt; std::endl;\n    }\n\n    MPI_Finalize();\n    return 0;\n}\n</code></pre> <p>Necesita 1199 iteraciones.</p>"},{"location":"codigo_cpp/Gauss_Seidel_Memloc/","title":"Soluci\u00f3n Paralela de la Ecuaci\u00f3n de Laplace con OpenMP","text":"<p>Aca se busca paralelizar con el uso de OpenMP, de manera que se trabaja en memoria compartida.</p>"},{"location":"codigo_cpp/Gauss_Seidel_Memloc/#codigo-fuente","title":"C\u00f3digo Fuente","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;cmath&gt; // Para std::abs\n#include &lt;algorithm&gt; // Para std::max\n#include &lt;omp.h&gt; // OpenMP\n#include &lt;sys/time.h&gt; // Para medir tiempo\n\ndouble seconds()\n{\n  struct timeval tmp;\n  double sec;\n  gettimeofday(&amp;tmp, (struct timezone *)0);\n  sec = tmp.tv_sec + ((double)tmp.tv_usec) / 1000000.0;\n\n  return sec;\n}\n\ndouble calcular_delta(const std::vector&lt;double&gt; &amp;phi, const std::vector&lt;double&gt; &amp;phi_copy) {\n    double delta = 0.0;\n    for (size_t i = 0; i &lt; phi.size(); ++i) {\n        delta = std::max(delta, std::abs(phi[i] - phi_copy[i]));\n    }\n    return delta;\n}\n\nint main(int argc, char* argv[]) {\n    if (argc != 7) {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" --N [Tama\u00f1o lineal de la grilla cuadrada (NxN)] --t [Tolerancia] --L [Tama\u00f1o lineal del capacitor cuadrado (LxL)]\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    std::cout &lt;&lt; \"Bienvenido.\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Se est\u00e1 calculando la soluci\u00f3n de la ecuaci\u00f3n de Laplace bajo los par\u00e1metros definidos...\" &lt;&lt; std::endl;\n\n    int N = atoi(argv[2]);\n    double tolerance = atof(argv[4]);\n    int L = atoi(argv[6]);\n    int grilla_size = N + 1;\n\n    std::vector&lt;double&gt; phi(grilla_size * grilla_size, 0.0);\n\n    int start_y = (N / L) * 2.0;\n    int end_y = (N / L) * 8.0;\n    int plate_x1 = (N / L) * 2.0;\n    int plate_x2 = (N / L) * 8.0;\n\n    for (int i = start_y; i &lt; end_y; ++i) {\n        phi[i * grilla_size + plate_x1] = 1.0;\n        phi[i * grilla_size + plate_x2] = -1.0;\n    }\n\n    std::vector&lt;double&gt; phi_copy = phi;\n\n    double delta = 1.0;\n    int its = 0;\n    int num_procs;\n    double time_1 = seconds();\n\n    while (delta &gt; tolerance) {\n        its++;\n#pragma omp parallel\n        {\n            num_procs = omp_get_num_threads();\n#pragma omp for\n            for (int i = 1; i &lt; N; ++i) {\n                for (int j = 1; j &lt; N; ++j) {\n                    if ((i &gt;= start_y &amp;&amp; i &lt;= end_y) &amp;&amp; (j == plate_x1 || j == plate_x2)) {\n                        // No se modifica la placa\n                    } else {\n                        phi[i * grilla_size + j] = (1.0 / 4.0) *\n                            (phi[(i + 1) * grilla_size + j] +\n                             phi[(i - 1) * grilla_size + j] +\n                             phi[i * grilla_size + (j + 1)] +\n                             phi[i * grilla_size + (j - 1)]);\n                    }\n                }\n            }\n        }\n\n        delta = calcular_delta(phi, phi_copy);\n        phi_copy = phi;\n    }\n\n    double time_2 = seconds();\n    std::cout &lt;&lt; \"Se lleg\u00f3 a la tolerancia tras \" &lt;&lt; its &lt;&lt; \" iteraciones.\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"N\u00famero de hilos: \" &lt;&lt; num_procs &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Tiempo total: \" &lt;&lt; time_2 - time_1 &lt;&lt; \" segundos\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre> <p>Converge tras 1143 iteraciones.</p> <p>Escalabilidad:</p> <p></p>"},{"location":"notebooks/jacobi_modificado/","title":"M\u00e9todo de Relajaci\u00f3n de Jacobi Modificado","text":"<p>Este c\u00f3digo pretende implementar una mejora al m\u00e9todo de Jacobi, a\u00f1adiendo un factor omega para acelerar la convergencia en sistemas lineales.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef jacobi_relaxation_mod(N, tolerance, L, V1, V2, omega):\n    # Inicializar la matriz phi (potencial el\u00e9ctrico)\n    phi = np.zeros((N + 1, N + 1), dtype=float)\n\n    # Aplicar condiciones de frontera\n    phi[int((N / L) * 2):int((N / L) * 8), int((N / L) * 2)] = V1\n    phi[int((N / L) * 2):int((N / L) * 8), int((N / L) * 8)] = V2\n\n    delta = 1.0\n    iteration = 0\n\n    while delta &gt; tolerance:\n        phi_new = phi.copy()\n        iteration += 1\n        phi_new[1:N, 1:N] = (1.0 + omega) * 0.25 * (\n            phi[2:N + 1, 1:N] + phi[0:N - 1, 1:N] + \n            phi[1:N, 2:N + 1] + phi[1:N, 0:N - 1]\n        ) - omega * phi[1:N, 1:N]\n\n        phi_new[int((N / L) * 2):int((N / L) * 8), int((N / L) * 2)] = V1\n        phi_new[int((N / L) * 2):int((N / L) * 8), int((N / L) * 8)] = V2\n\n        delta = np.max(np.abs(phi - phi_new))\n        phi = phi_new\n\n    print(f\"El m\u00e9todo convergi\u00f3 en {iteration} iteraciones.\")\n    return phi, iteration\n\nphi, num_iterations = jacobi_relaxation_mod(100, 1e-3, 10, 1, -1, 1.5)\n\n\nEl m\u00e9todo convergi\u00f3 en 523 iteraciones.\n\n/tmp/ipykernel_5055/757925980.py:23: RuntimeWarning: overflow encountered in add\n  phi[2:N + 1, 1:N] + phi[0:N - 1, 1:N] +\n/tmp/ipykernel_5055/757925980.py:25: RuntimeWarning: overflow encountered in multiply\n  ) - omega * phi[1:N, 1:N]\n/tmp/ipykernel_5055/757925980.py:22: RuntimeWarning: overflow encountered in subtract\n  phi_new[1:N, 1:N] = (1.0 + omega) * 0.25 * (\n/tmp/ipykernel_5055/757925980.py:32: RuntimeWarning: overflow encountered in subtract\n  delta = np.max(np.abs(phi - phi_new))\n/tmp/ipykernel_5055/757925980.py:23: RuntimeWarning: invalid value encountered in add\n  phi[2:N + 1, 1:N] + phi[0:N - 1, 1:N] +\n\n# Graficar el resultado despu\u00e9s de la convergencia, en este caso diverge este m\u00e9todo.\n#plt.figure(figsize=(8, 6))\n#plt.imshow(phi, extent=[0, 10, 0, 10], origin='lower', cmap='viridis', interpolation='bilinear')\n#plt.colorbar(label='Potencial el\u00e9ctrico (phi)')\n#plt.title(f'Distribuci\u00f3n del Potencial El\u00e9ctrico - Iteraciones: {num_iterations}')\n#plt.xlabel('x')\n#plt.ylabel('y')\n#plt.show()\n\n</code></pre> <p>Se encuentra que para este problema en especifico, la soluci\u00f3n bajo Jacobi modificado siempre diverge sin importar el valor de omega.</p>"},{"location":"scripts/Gauss_Seidel/","title":"M\u00e9todo de Gauss-Seidel","text":"<p>En este script se implementa el m\u00e9todo de Gauss-Seidel, en el que se busca iterar sobre el mismo campo en memoria, lo que permite una convergencia mas rapida. De todas maneras, por la naturaleza del m\u00e9todo, es necesario usar <code>for</code> loops, lo que afecta la velocidad.</p>"},{"location":"scripts/Gauss_Seidel/#codigo-en-python","title":"C\u00f3digo en Python","text":"<pre><code>#!/usr/bin/env python\n\nimport numpy as np\n\ndef gauss_seidel(N=100, tolerance=1e-5, L=10, V1=1, V2=-1):\n    # Iniciaci\u00f3n de phi\n    phi = np.zeros((N + 1, N + 1), dtype=float)\n\n    # Condiciones iniciales (placas)\n    phi[int((N/L)*2):int((N/L)*8), int((N/L)*2)] = V1\n    phi[int((N/L)*2):int((N/L)*8), int((N/L)*8)] = V2\n\n    # Copia para evaluar el error\n    phi_copy = phi.copy()\n\n    delta = 1.0\n    its = 0\n\n    while delta &gt; tolerance:\n        its += 1\n\n        # Iteraci\u00f3n de Gauss-Seidel\n        for i in range(1, N):\n            for j in range(1, N):\n                phi[i, j] = (phi[i+1, j] + phi[i-1, j] + phi[i, j+1] + phi[i, j-1]) / 4.0\n\n        # Restauro el potencial en las placas\n        phi[int((N / L) * 2):int((N / L) * 8), int((N / L) * 2)] = V1\n        phi[int((N / L) * 2):int((N / L) * 8), int((N / L) * 8)] = V2\n\n        # Evaluar la convergencia\n        delta = np.max(np.abs(phi - phi_copy))\n        phi_copy = phi.copy()\n\n    return phi, its\n\ngauss_vals, iterations = gauss_seidel()\nprint(iterations)\n</code></pre> <p>Este m\u00e9todo converge en 1196 iteraciones, una gran mejora respecto a Jacobi (1803 iteraciones).</p> <p>Gr\u00e1fico </p>"},{"location":"scripts/Relajacion_Jacobi/","title":"Relajaci\u00f3n Jacobi","text":"<p>Este script pretende implementar el m\u00e9todo de relajaci\u00f3n de Jacobi para resolver la ecuaci\u00f3n de Laplace. Es \u00fatil en aplicaciones como simulaciones de potencial el\u00e9ctrico, y facilita la representaci\u00f3n grafica de la soluci\u00f3n.</p>"},{"location":"scripts/Relajacion_Jacobi/#codigo","title":"C\u00f3digo","text":"<pre><code>import numpy as np\ndef jacobi_relaxation(N=100, tolerance=1e-5, L=10, V1=1, V2=-1):\n    # Iniciaci\u00f3n de phi\n    phi = np.zeros((N + 1, N + 1), dtype=float)\n    # Condiciones iniciales (placas)\n    phi[int((N/L)*2):int((N/L)*8), int((N/L)*2)] = V1\n    phi[int((N/L)*2):int((N/L)*8), int((N/L)*8)] = V2\n\n    delta = 1.0\n    its = 0\n\n    while delta &gt; tolerance:\n        its += 1\n        phi_new = phi.copy()\n        phi_new[1:N, 1:N] = (1.0/4.0) * (\n            phi[2:N + 1, 1:N] +\n            phi[0:N - 1, 1:N] +\n            phi[1:N, 2:N + 1] +\n            phi[1:N, 0:N - 1]\n        )\n        # Restauro el potencial en las placas\n        phi_new[int((N / L) * 2):int((N / L) * 8), int((N / L) * 2)] = V1\n        phi_new[int((N / L) * 2):int((N / L) * 8), int((N / L) * 8)] = V2\n\n        delta = np.max(np.abs(phi - phi_new))\n        phi = phi_new\n\n    return phi, its\n\njacobi_vals, iterations = jacobi_relaxation()\nprint(iterations)\n\nEn el jupyter:\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nplt.imshow(jacobi_vals)\nplt.gray()\nplt.show()\n</code></pre> <p>Converge tras 1803 iteraciones.</p> <p>Gr\u00e1fico </p>"}]}